{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Optimizing License Plate Recognition - Pytorch model"
      ],
      "metadata": {
        "id": "E090Ye4hiilV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision onnx onnxruntime tvm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuUNuhWvgM0_",
        "outputId": "636b0e12-f5d7-4f33-a71e-12510c602a81"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting tvm\n",
            "  Downloading tvm-1.0.0.tar.gz (5.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n",
            "Collecting appdirs (from tvm)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting docopt (from tvm)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting inform (from tvm)\n",
            "  Downloading inform-1.32-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting quantiphy (from tvm)\n",
            "  Downloading quantiphy-2.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting arrow (from inform->tvm)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from inform->tvm) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->inform->tvm) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->inform->tvm)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inform-1.32-py3-none-any.whl (39 kB)\n",
            "Downloading quantiphy-2.20-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: tvm, docopt\n",
            "  Building wheel for tvm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tvm: filename=tvm-1.0.0-py3-none-any.whl size=5084 sha256=fe70405a9e73150bd1c28dbf0aa61381580058f23ef1264eb035d1f3c626c28c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/31/8c/025d5271ffd5a09fa26522edb4cdbb3d532c2f254a3bbb7c40\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=fe07b93de5989c1b844b98a47af1daa7222b914c91ca036346ce2afc10b355f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built tvm docopt\n",
            "Installing collected packages: docopt, appdirs, types-python-dateutil, quantiphy, onnx, humanfriendly, coloredlogs, arrow, onnxruntime, inform, tvm\n",
            "Successfully installed appdirs-1.4.4 arrow-1.3.0 coloredlogs-15.0.1 docopt-0.6.2 humanfriendly-10.0 inform-1.32 onnx-1.17.0 onnxruntime-1.20.1 quantiphy-2.20 tvm-1.0.0 types-python-dateutil-2.9.0.20241206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"GPU Available:\", torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WomsXG8egQuZ",
        "outputId": "93aafd0e-51bd-4827-9f52-5c49875d1848"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leN6MHitZxxR",
        "outputId": "f0f1cf99-49c5-4a38-fe2b-1317c2d6a312"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://mlc.ai/wheels\n",
            "Collecting mlc-ai-cpu\n",
            "  Downloading https://github.com/mlc-ai/package/releases/download/v0.9.dev0/mlc_ai_cpu-0.17.2-cp310-cp310-manylinux_2_28_x86_64.whl (185.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.8/185.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (24.2.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (3.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (4.4.2)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (0.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (1.13.1)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (6.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from mlc-ai-cpu) (4.12.2)\n",
            "Installing collected packages: mlc-ai-cpu\n",
            "Successfully installed mlc-ai-cpu-0.17.2\n"
          ]
        }
      ],
      "source": [
        "!python3 -m  pip install mlc-ai-cpu -f https://mlc.ai/wheels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sirius-ai/LPRNet_Pytorch.git\n",
        "%cd LPRNet_Pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H15nv6CqaBKh",
        "outputId": "ebdc1ffb-0beb-440a-81c1-f7ef0fadf7a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LPRNet_Pytorch'...\n",
            "remote: Enumerating objects: 1071, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 1071 (delta 25), reused 22 (delta 22), pack-reused 1037 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1071/1071), 20.04 MiB | 16.16 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "/content/LPRNet_Pytorch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_LPRNet.py --test_img_dirs ./data/test --pretrained_model ./weights/Final_LPRNet_model.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcI7z1MIY23D",
        "outputId": "5c83b2ad-f615-4d36-df94-384e54b3ef66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n",
            "/content/LPRNet_Pytorch/test_LPRNet.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model))\n",
            "load pretrained model successful!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.897 [897:63:40:1000]\n",
            "[Info] Test Speed: 0.0024281809329986573s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "hBuYfyGcaFAU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original LPRNet model"
      ],
      "metadata": {
        "id": "JMqa2DJCCVNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class small_basic_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(small_basic_block, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(ch_in, ch_out // 4, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out, kernel_size=1),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "class LPRNet(nn.Module):\n",
        "    def __init__(self, lpr_max_len, phase, class_num, dropout_rate):\n",
        "        super(LPRNet, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.lpr_max_len = lpr_max_len\n",
        "        self.class_num = class_num\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1), # 0\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),  # 2\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1)),\n",
        "            small_basic_block(ch_in=64, ch_out=128),    # *** 4 ***\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(),  # 6\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2)),\n",
        "            small_basic_block(ch_in=64, ch_out=256),   # 8\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),  # 10\n",
        "            small_basic_block(ch_in=256, ch_out=256),   # *** 11 ***\n",
        "            nn.BatchNorm2d(num_features=256),   # 12\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2)),  # 14\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(1, 4), stride=1),  # 16\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),  # 18\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=256, out_channels=class_num, kernel_size=(13, 1), stride=1), # 20\n",
        "            nn.BatchNorm2d(num_features=class_num),\n",
        "            nn.ReLU(),  # *** 22 ***\n",
        "        )\n",
        "        self.container = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=448+self.class_num, out_channels=self.class_num, kernel_size=(1, 1), stride=(1, 1)),\n",
        "            # nn.BatchNorm2d(num_features=self.class_num),\n",
        "            # nn.ReLU(),\n",
        "            # nn.Conv2d(in_channels=self.class_num, out_channels=self.lpr_max_len+1, kernel_size=3, stride=2),\n",
        "            # nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        keep_features = list()\n",
        "        for i, layer in enumerate(self.backbone.children()):\n",
        "            x = layer(x)\n",
        "            if i in [2, 6, 13, 22]: # [2, 4, 8, 11, 22]\n",
        "                keep_features.append(x)\n",
        "\n",
        "        global_context = list()\n",
        "        for i, f in enumerate(keep_features):\n",
        "            if i in [0, 1]:\n",
        "                f = nn.AvgPool2d(kernel_size=5, stride=5)(f)\n",
        "            if i in [2]:\n",
        "                f = nn.AvgPool2d(kernel_size=(4, 10), stride=(4, 2))(f)\n",
        "            f_pow = torch.pow(f, 2)\n",
        "            f_mean = torch.mean(f_pow)\n",
        "            f = torch.div(f, f_mean)\n",
        "            global_context.append(f)\n",
        "\n",
        "        x = torch.cat(global_context, 1)\n",
        "        x = self.container(x)\n",
        "        logits = torch.mean(x, dim=2)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def build_lprnet(lpr_max_len=8, phase=False, class_num=66, dropout_rate=0.5):\n",
        "\n",
        "    Net = LPRNet(lpr_max_len, phase, class_num, dropout_rate)\n",
        "\n",
        "    if phase == \"train\":\n",
        "        return Net.train()\n",
        "    else:\n",
        "        return Net.eval()"
      ],
      "metadata": {
        "id": "joj37wW8aHmY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up test function -\n",
        "(similar to test_LPRNet.py from the LPRNet repository)"
      ],
      "metadata": {
        "id": "nJW5cEitC4a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from data.load_data import CHARS, CHARS_DICT, LPRDataLoader\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from model.LPRNet import build_lprnet\n",
        "# import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import *\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import argparse\n",
        "import torch\n",
        "import time\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.img_size = [94, 24]\n",
        "        self.test_img_dirs = \"./data/test\"\n",
        "        self.dropout_rate = 0\n",
        "        self.lpr_max_len = 8\n",
        "        self.test_batch_size = 100\n",
        "        self.phase_train = False\n",
        "        self.num_workers = 8\n",
        "        self.cuda = False\n",
        "        self.show = False\n",
        "        self.pretrained_model = './weights/Final_LPRNet_model.pth'\n",
        "\n",
        "args = Config()\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for _, sample in enumerate(batch):\n",
        "        img, label, length = sample\n",
        "        imgs.append(torch.from_numpy(img))\n",
        "        labels.extend(label)\n",
        "        lengths.append(length)\n",
        "    labels = np.asarray(labels).flatten().astype(np.float32)\n",
        "\n",
        "    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)\n",
        "\n",
        "def test(module):\n",
        "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "    Greedy_Decode_Eval_tvm(module, test_dataset, args)\n",
        "\n",
        "\n",
        "def Greedy_Decode_Eval_tvm(module, datasets, args):\n",
        "    # TestNet = Net.eval()\n",
        "    epoch_size = len(datasets) // args.test_batch_size\n",
        "    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
        "\n",
        "    Tp = 0\n",
        "    Tn_1 = 0\n",
        "    Tn_2 = 0\n",
        "    t1 = time.time()\n",
        "    for i in range(epoch_size):\n",
        "        # load train data\n",
        "        images, labels, lengths = next(batch_iterator)\n",
        "        start = 0\n",
        "        targets = []\n",
        "        for length in lengths:\n",
        "            label = labels[start:start+length]\n",
        "            targets.append(label)\n",
        "            start += length\n",
        "        targets = np.array([el.numpy() for el in targets])\n",
        "        imgs = images.numpy().copy()\n",
        "\n",
        "        if args.cuda:\n",
        "            images = Variable(images.cuda())\n",
        "        else:\n",
        "            images = Variable(images)\n",
        "\n",
        "        module.set_input(input_name, tvm.nd.array(images.numpy()))\n",
        "        module.run()\n",
        "        tvm_output = module.get_output(0).asnumpy()\n",
        "        prebs = tvm_output\n",
        "        preb_labels = list()\n",
        "        for i in range(prebs.shape[0]):\n",
        "            preb = prebs[i, :, :]\n",
        "            preb_label = list()\n",
        "            for j in range(preb.shape[1]):\n",
        "                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
        "            no_repeat_blank_label = list()\n",
        "            pre_c = preb_label[0]\n",
        "            if pre_c != len(CHARS) - 1:\n",
        "                no_repeat_blank_label.append(pre_c)\n",
        "            for c in preb_label: # dropout repeate label and blank label\n",
        "                if (pre_c == c) or (c == len(CHARS) - 1):\n",
        "                    if c == len(CHARS) - 1:\n",
        "                        pre_c = c\n",
        "                    continue\n",
        "                no_repeat_blank_label.append(c)\n",
        "                pre_c = c\n",
        "            preb_labels.append(no_repeat_blank_label)\n",
        "        for i, label in enumerate(preb_labels):\n",
        "            if len(label) != len(targets[i]):\n",
        "                Tn_1 += 1\n",
        "                continue\n",
        "            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
        "                Tp += 1\n",
        "            else:\n",
        "                Tn_2 += 1\n",
        "    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
        "    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp+Tn_1+Tn_2)))\n",
        "    t2 = time.time()\n",
        "    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))\n"
      ],
      "metadata": {
        "id": "jf4D91-0Db5c"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original Model Size"
      ],
      "metadata": {
        "id": "yFd3GY1fPToK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from model.LPRNet import build_lprnet  # Ensure this matches your file path\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "# Initialize the original model and wrap it\n",
        "lprnet_model = build_lprnet()  # Load the LPRNet model\n",
        "\n",
        "# Now you can export the wrapped model to ONNX\n",
        "dummy_input = torch.randn(1, 3, 24, 94,device='cuda')  # Dummy input for ONNX export\n",
        "lprnet_model.to(dummy_input.device)\n",
        "onnx_model_path = './weights/lprnet_model.onnx'\n",
        "torch.onnx.export(lprnet_model, dummy_input, onnx_model_path, weight_type=QuantType.QInt8)\n",
        "print(f'Model exported to: {onnx_model_path}')\n",
        "\n",
        "import os\n",
        "\n",
        "# Get the file size of the ONNX model\n",
        "onnx_size = os.path.getsize(onnx_model_path) / (1024 * 1024)  # Size in MB\n",
        "print(f\"ONNX model size: {onnx_size:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTP51nKbPVio",
        "outputId": "dcb5c4e5-d4c1-4ece-cc86-e155157f707b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to: ./weights/lprnet_model.onnx\n",
            "ONNX model size: 1.68 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLC Optimization"
      ],
      "metadata": {
        "id": "9ocEhl_sHaJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up model to perform MLC optimizations"
      ],
      "metadata": {
        "id": "X3xDhSSZHdWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "from tvm import relay\n",
        "import onnx\n",
        "from tvm.contrib import graph_executor\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 24, 94)\n",
        "lprnet = build_lprnet(lpr_max_len=8, phase=False, class_num=68, dropout_rate=0.5)\n",
        "lprnet.load_state_dict(torch.load(\"./weights/Final_LPRNet_model.pth\",  map_location=torch.device('cpu')))\n",
        "lprnet.eval()\n",
        "\n",
        "relay_model = torch.jit.trace(lprnet,dummy_input).eval()\n",
        "relay_model.save(\"jit_traced_lprnet.pt\")\n",
        "\n",
        "# Load the model\n",
        "relay_model = torch.jit.load(\"jit_traced_lprnet.pt\")\n",
        "relay_model.eval()\n",
        "\n",
        "# Define the input shape for the model\n",
        "input_name = \"input0\"\n",
        "input_shape = (100, 3, 24, 94)\n",
        "shape_dict = [(input_name,input_shape)]\n",
        "\n",
        "# Convert ONNX model to TVM Relay format\n",
        "relay_mod, params = relay.frontend.from_pytorch(relay_model, shape_dict)\n",
        "\n",
        "# Specify target (e.g., NVIDIA GPU or CPU)\n",
        "target = \"llvm\"  # Use \"llvm\" for CPU\n",
        "dev = tvm.cuda(0) if target == \"cuda\" else tvm.cpu()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKI53K7IaWgs",
        "outputId": "b96fda28-87e4-4380-cbd7-d93434f6a954"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-9fa621fb8eed>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(\"./weights/Final_LPRNet_model.pth\",  map_location=torch.device('cpu')))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm import relay\n",
        "\n",
        "# Start with type inference and simplification\n",
        "mod = relay.transform.InferType()(relay_mod)\n",
        "mod = relay.transform.SimplifyInference()(mod)\n",
        "\n",
        "# Perform advanced optimizations\n",
        "mod = relay.transform.FoldScaleAxis()(mod)\n",
        "mod = relay.transform.FuseOps()(mod)\n",
        "mod = relay.transform.AlterOpLayout()(mod)\n",
        "\n",
        "mod = relay.transform.EliminateCommonSubexpr()(mod)\n",
        "\n",
        "# Final cleanup and memory optimization\n",
        "mod = relay.transform.DeadCodeElimination()(mod)"
      ],
      "metadata": {
        "id": "2eRCw0fGazqP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target device\n",
        "target = \"llvm\"\n",
        "dev = tvm.cuda(0) if target == \"cuda\" else tvm.cpu()\n",
        "\n",
        "# Compile the model\n",
        "with tvm.transform.PassContext(opt_level=3):\n",
        "    lib = relay.build(mod, target=target, params=params)\n",
        "\n",
        "module = graph_executor.GraphModule(lib[\"default\"](dev))\n",
        "test(module)"
      ],
      "metadata": {
        "id": "AiaMy6c9a2Sr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d59e87-50f8-4529-c754-65158e73105b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autotvm:One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.903 [903:57:40:1000]\n",
            "[Info] Test Speed: 0.03870338368415833s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Perform TVM Autotune"
      ],
      "metadata": {
        "id": "V8hpqnROa8Sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm.autotvm.tuner import XGBTuner\n",
        "from tvm import autotvm"
      ],
      "metadata": {
        "id": "xdMa0rq0a8_b"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "model = onnx.load(onnx_model_path)\n",
        "\n",
        "# Get input names\n",
        "input_all = [node.name for node in model.graph.input]\n",
        "input_initializer = [node.name for node in model.graph.initializer]\n",
        "net_feed_input = list(set(input_all) - set(input_initializer))\n",
        "\n",
        "print(\"Input names:\", net_feed_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0IygPHMP6Lu",
        "outputId": "530aefd3-e40a-4efd-f88e-b5ca451e2814"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input names: ['input.1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm.autotvm.tuner import XGBTuner\n",
        "from tvm.autotvm import task, measure\n",
        "import tvm.auto_scheduler as auto_scheduler\n",
        "\n",
        "# Extract tasks for tuning\n",
        "tasks = tvm.autotvm.task.extract_from_program(\n",
        "    relay_mod[\"main\"], target=target, params=params\n",
        ")\n",
        "\n",
        "# Check the extracted tasks\n",
        "print(f\"Extracted {len(tasks)} tasks for tuning.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PujglC0zQlyf",
        "outputId": "44dbd846-4353-42c8-8046-aa93efa126da"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 13 tasks for tuning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tvm.autotvm.measure.measure_methods import LocalBuilder, LocalRunner\n",
        "\n",
        "# Define the measure option for local execution\n",
        "measure_option = tvm.autotvm.measure_option(\n",
        "    builder=LocalBuilder(),\n",
        "    runner=LocalRunner(number=10,repeat=1, timeout=10, min_repeat_ms=100)\n",
        ")\n",
        "\n",
        "tuning_option = {\n",
        "    \"log_filename\": \"autotvm_tuning.log\",\n",
        "    \"tuner\": \"xgb\",  # Use XGBoost tuner\n",
        "    \"n_trial\": 20,  # Reduce number of trials\n",
        "    \"early_stopping\": 50,  # Stop earlier if no improvements\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "# Perform tuning\n",
        "for i, tsk in enumerate(tasks):\n",
        "    print(f\"Tuning task {i + 1}/{len(tasks)}\")\n",
        "    tuner = XGBTuner(tsk)\n",
        "    tuner.tune(\n",
        "        n_trial=tuning_option[\"n_trial\"],\n",
        "        early_stopping=tuning_option[\"early_stopping\"],\n",
        "\n",
        "        measure_option=measure_option,\n",
        "        callbacks=[\n",
        "            tvm.autotvm.callback.log_to_file(tuning_option[\"log_filename\"]),\n",
        "            tvm.autotvm.callback.progress_bar(tuning_option[\"n_trial\"]),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "# Apply the best tuning results\n",
        "with tvm.autotvm.apply_history_best(tuning_option[\"log_filename\"]):\n",
        "    with tvm.transform.PassContext(opt_level=3):\n",
        "        lib = relay.build(relay_mod, target=target, params=params)\n",
        "\n",
        "# Save the graph and parameters\n",
        "with open(\"graph.json\", \"w\") as f:\n",
        "    f.write(lib.get_graph_json())  # Save the model graph\n",
        "\n",
        "with open(\"params.params\", \"wb\") as f:\n",
        "    f.write(tvm.runtime.save_param_dict(params))  # Save the model parameters\n",
        "\n",
        "\n",
        "# Save the compiled module (optional)\n",
        "lib.export_library(\"optimized_model.tar\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8h4gTSNQzT9",
        "outputId": "6d1b2c5d-cd60-4950-f2e5-3b54973663be"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning task 1/13\n",
            " Current/Best:    6.18/  17.26 GFLOPS | Progress: (20/20) | 46.29 s Done.\n",
            "Tuning task 2/13\n",
            " Current/Best:    3.34/  21.32 GFLOPS | Progress: (20/20) | 48.14 s Done.\n",
            "Tuning task 3/13\n",
            " Current/Best:    2.19/  16.09 GFLOPS | Progress: (20/20) | 63.63 s Done.\n",
            "Tuning task 4/13\n",
            " Current/Best:   14.13/  22.47 GFLOPS | Progress: (20/20) | 45.70 s Done.\n",
            "Tuning task 5/13\n",
            " Current/Best:   10.23/  18.65 GFLOPS | Progress: (20/20) | 74.39 s Done.\n",
            "Tuning task 6/13\n",
            " Current/Best:    6.98/  20.78 GFLOPS | Progress: (20/20) | 31.26 s Done.\n",
            "Tuning task 7/13\n",
            " Current/Best:   13.43/  16.27 GFLOPS | Progress: (20/20) | 85.41 s Done.\n",
            "Tuning task 8/13\n",
            " Current/Best:    6.71/  21.99 GFLOPS | Progress: (20/20) | 56.99 s Done.\n",
            "Tuning task 9/13\n",
            " Current/Best:    9.10/  14.97 GFLOPS | Progress: (20/20) | 104.99 s Done.\n",
            "Tuning task 10/13\n",
            " Current/Best:   13.96/  19.62 GFLOPS | Progress: (20/20) | 88.28 s Done.\n",
            "Tuning task 11/13\n",
            " Current/Best:    8.15/  19.56 GFLOPS | Progress: (20/20) | 100.92 s Done.\n",
            "Tuning task 12/13\n",
            " Current/Best:    6.13/  12.50 GFLOPS | Progress: (20/20) | 189.88 s Done.\n",
            "Tuning task 13/13\n",
            " Current/Best:   13.99/  13.99 GFLOPS | Progress: (20/20) | 48.69 s Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with autotvm.apply_history_best(tuning_option[\"log_filename\"]):\n",
        "    with tvm.transform.PassContext(opt_level=3, config={}):\n",
        "        lib = relay.build(mod, target=target, params=params)\n",
        "\n",
        "dev = tvm.device(str(target), 0)\n",
        "module = graph_executor.GraphModule(lib[\"default\"](dev))"
      ],
      "metadata": {
        "id": "OpvGPowHbOnb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0bkILaQbRqU",
        "outputId": "2aa85eee-d3f6-4c83-a96a-dc2c422b93fa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.899 [899:61:40:1000]\n",
            "[Info] Test Speed: 0.03080963134765625s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from model.LPRNet import build_lprnet  # Ensure this matches your file path\n",
        "\n",
        "# Initialize the original model and wrap it\n",
        "lprnet_model = build_lprnet()  # Load the LPRNet model\n",
        "\n",
        "# Now you can export the wrapped model to ONNX\n",
        "dummy_input = torch.randn(1, 3, 24, 94,device='cuda')  # Dummy input for ONNX export\n",
        "lprnet_model.to(dummy_input.device)\n",
        "mlc_optimized_onnx_model_path = './optimized_model.tar'\n",
        "torch.onnx.export(lprnet_model, dummy_input, mlc_optimized_onnx_model_path, weight_type=QuantType.QInt8)\n",
        "print(f'Model exported to: {mlc_optimized_onnx_model_path}')\n",
        "\n",
        "import os\n",
        "\n",
        "# Get the file size of the ONNX model\n",
        "mlc_onnx_size = os.path.getsize(mlc_optimized_onnx_model_path) / (1024 * 1024)  # Size in MB\n",
        "print(f\"ONNX model size: {mlc_onnx_size:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_6uEut1jqnn",
        "outputId": "22797c9b-f2c8-4278-e7e7-54cc482eb9e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to: ./optimized_model.tar\n",
            "ONNX model size: 1.68 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODEL OPTIMIZATION"
      ],
      "metadata": {
        "id": "fO7EapTUgXR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the script content with line numbers\n",
        "!cat -n test_LPRNet.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usOQpQoQggeq",
        "outputId": "e0fe91f3-4de5-4a6a-d155-fb14e8bec398"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     1\t# -*- coding: utf-8 -*-\n",
            "     2\t# /usr/bin/env/python3\n",
            "     3\t\n",
            "     4\t'''\n",
            "     5\ttest pretrained model.\n",
            "     6\tAuthor: aiboy.wei@outlook.com .\n",
            "     7\t'''\n",
            "     8\t\n",
            "     9\tfrom data.load_data import CHARS, CHARS_DICT, LPRDataLoader\n",
            "    10\tfrom PIL import Image, ImageDraw, ImageFont\n",
            "    11\tfrom model.LPRNet import build_lprnet\n",
            "    12\t# import torch.backends.cudnn as cudnn\n",
            "    13\tfrom torch.autograd import Variable\n",
            "    14\timport torch.nn.functional as F\n",
            "    15\tfrom torch.utils.data import *\n",
            "    16\tfrom torch import optim\n",
            "    17\timport torch.nn as nn\n",
            "    18\timport numpy as np\n",
            "    19\timport argparse\n",
            "    20\timport torch\n",
            "    21\timport time\n",
            "    22\timport cv2\n",
            "    23\timport os\n",
            "    24\t\n",
            "    25\tdef get_parser():\n",
            "    26\t    parser = argparse.ArgumentParser(description='parameters to train net')\n",
            "    27\t    parser.add_argument('--img_size', default=[94, 24], help='the image size')\n",
            "    28\t    parser.add_argument('--test_img_dirs', default=\"./data/test\", help='the test images path')\n",
            "    29\t    parser.add_argument('--dropout_rate', default=0, help='dropout rate.')\n",
            "    30\t    parser.add_argument('--lpr_max_len', default=8, help='license plate number max length.')\n",
            "    31\t    parser.add_argument('--test_batch_size', default=100, help='testing batch size.')\n",
            "    32\t    parser.add_argument('--phase_train', default=False, type=bool, help='train or test phase flag.')\n",
            "    33\t    parser.add_argument('--num_workers', default=8, type=int, help='Number of workers used in dataloading')\n",
            "    34\t    parser.add_argument('--cuda', default=True, type=bool, help='Use cuda to train model')\n",
            "    35\t    parser.add_argument('--show', default=False, type=bool, help='show test image and its predict result or not.')\n",
            "    36\t    parser.add_argument('--pretrained_model', default='./weights/Final_LPRNet_model.pth', help='pretrained base model')\n",
            "    37\t\n",
            "    38\t    args = parser.parse_args()\n",
            "    39\t\n",
            "    40\t    return args\n",
            "    41\t\n",
            "    42\tdef collate_fn(batch):\n",
            "    43\t    imgs = []\n",
            "    44\t    labels = []\n",
            "    45\t    lengths = []\n",
            "    46\t    for _, sample in enumerate(batch):\n",
            "    47\t        img, label, length = sample\n",
            "    48\t        imgs.append(torch.from_numpy(img))\n",
            "    49\t        labels.extend(label)\n",
            "    50\t        lengths.append(length)\n",
            "    51\t    labels = np.asarray(labels).flatten().astype(np.float32)\n",
            "    52\t\n",
            "    53\t    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)\n",
            "    54\t\n",
            "    55\tdef test():\n",
            "    56\t    args = get_parser()\n",
            "    57\t\n",
            "    58\t    lprnet = build_lprnet(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
            "    59\t    device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
            "    60\t    lprnet.to(device)\n",
            "    61\t    print(\"Successful to build network!\")\n",
            "    62\t\n",
            "    63\t    # load pretrained model\n",
            "    64\t    if args.pretrained_model:\n",
            "    65\t        lprnet.load_state_dict(torch.load(args.pretrained_model))\n",
            "    66\t        print(\"load pretrained model successful!\")\n",
            "    67\t    else:\n",
            "    68\t        print(\"[Error] Can't found pretrained mode, please check!\")\n",
            "    69\t        return False\n",
            "    70\t\n",
            "    71\t    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
            "    72\t    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
            "    73\t    try:\n",
            "    74\t        Greedy_Decode_Eval(lprnet, test_dataset, args)\n",
            "    75\t    finally:\n",
            "    76\t        cv2.destroyAllWindows()\n",
            "    77\t\n",
            "    78\tdef Greedy_Decode_Eval(Net, datasets, args):\n",
            "    79\t    # TestNet = Net.eval()\n",
            "    80\t    epoch_size = len(datasets) // args.test_batch_size\n",
            "    81\t    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
            "    82\t\n",
            "    83\t    Tp = 0\n",
            "    84\t    Tn_1 = 0\n",
            "    85\t    Tn_2 = 0\n",
            "    86\t    t1 = time.time()\n",
            "    87\t    for i in range(epoch_size):\n",
            "    88\t        # load train data\n",
            "    89\t        images, labels, lengths = next(batch_iterator)\n",
            "    90\t        start = 0\n",
            "    91\t        targets = []\n",
            "    92\t        for length in lengths:\n",
            "    93\t            label = labels[start:start+length]\n",
            "    94\t            targets.append(label)\n",
            "    95\t            start += length\n",
            "    96\t        targets = np.array([el.numpy() for el in targets])\n",
            "    97\t        imgs = images.numpy().copy()\n",
            "    98\t\n",
            "    99\t        if args.cuda:\n",
            "   100\t            images = Variable(images.cuda())\n",
            "   101\t        else:\n",
            "   102\t            images = Variable(images)\n",
            "   103\t\n",
            "   104\t        # forward\n",
            "   105\t        prebs = Net(images)\n",
            "   106\t        # greedy decode\n",
            "   107\t        prebs = prebs.cpu().detach().numpy()\n",
            "   108\t        preb_labels = list()\n",
            "   109\t        for i in range(prebs.shape[0]):\n",
            "   110\t            preb = prebs[i, :, :]\n",
            "   111\t            preb_label = list()\n",
            "   112\t            for j in range(preb.shape[1]):\n",
            "   113\t                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
            "   114\t            no_repeat_blank_label = list()\n",
            "   115\t            pre_c = preb_label[0]\n",
            "   116\t            if pre_c != len(CHARS) - 1:\n",
            "   117\t                no_repeat_blank_label.append(pre_c)\n",
            "   118\t            for c in preb_label: # dropout repeate label and blank label\n",
            "   119\t                if (pre_c == c) or (c == len(CHARS) - 1):\n",
            "   120\t                    if c == len(CHARS) - 1:\n",
            "   121\t                        pre_c = c\n",
            "   122\t                    continue\n",
            "   123\t                no_repeat_blank_label.append(c)\n",
            "   124\t                pre_c = c\n",
            "   125\t            preb_labels.append(no_repeat_blank_label)\n",
            "   126\t        for i, label in enumerate(preb_labels):\n",
            "   127\t            # show image and its predict label\n",
            "   128\t            if args.show:\n",
            "   129\t                show(imgs[i], label, targets[i])\n",
            "   130\t            if len(label) != len(targets[i]):\n",
            "   131\t                Tn_1 += 1\n",
            "   132\t                continue\n",
            "   133\t            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
            "   134\t                Tp += 1\n",
            "   135\t            else:\n",
            "   136\t                Tn_2 += 1\n",
            "   137\t    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
            "   138\t    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp+Tn_1+Tn_2)))\n",
            "   139\t    t2 = time.time()\n",
            "   140\t    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))\n",
            "   141\t\n",
            "   142\tdef show(img, label, target):\n",
            "   143\t    img = np.transpose(img, (1, 2, 0))\n",
            "   144\t    img *= 128.\n",
            "   145\t    img += 127.5\n",
            "   146\t    img = img.astype(np.uint8)\n",
            "   147\t\n",
            "   148\t    lb = \"\"\n",
            "   149\t    for i in label:\n",
            "   150\t        lb += CHARS[i]\n",
            "   151\t    tg = \"\"\n",
            "   152\t    for j in target.tolist():\n",
            "   153\t        tg += CHARS[int(j)]\n",
            "   154\t\n",
            "   155\t    flag = \"F\"\n",
            "   156\t    if lb == tg:\n",
            "   157\t        flag = \"T\"\n",
            "   158\t    # img = cv2.putText(img, lb, (0,16), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.6, (0, 0, 255), 1)\n",
            "   159\t    img = cv2ImgAddText(img, lb, (0, 0))\n",
            "   160\t    cv2.imshow(\"test\", img)\n",
            "   161\t    print(\"target: \", tg, \" ### {} ### \".format(flag), \"predict: \", lb)\n",
            "   162\t    cv2.waitKey()\n",
            "   163\t    cv2.destroyAllWindows()\n",
            "   164\t\n",
            "   165\tdef cv2ImgAddText(img, text, pos, textColor=(255, 0, 0), textSize=12):\n",
            "   166\t    if (isinstance(img, np.ndarray)):  # detect opencv format or not\n",
            "   167\t        img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
            "   168\t    draw = ImageDraw.Draw(img)\n",
            "   169\t    fontText = ImageFont.truetype(\"data/NotoSansCJK-Regular.ttc\", textSize, encoding=\"utf-8\")\n",
            "   170\t    draw.text(pos, text, textColor, font=fontText)\n",
            "   171\t\n",
            "   172\t    return cv2.cvtColor(np.asarray(img), cv2.COLOR_RGB2BGR)\n",
            "   173\t\n",
            "   174\t\n",
            "   175\tif __name__ == \"__main__\":\n",
            "   176\t    test()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"test_LPRNet.py\"\n",
        "with open(file_path, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "with open(file_path, \"w\") as file:\n",
        "    inside_finally = False\n",
        "    for line in lines:\n",
        "        if line.strip() == 'finally:':\n",
        "            inside_finally = True\n",
        "            file.write(line)  # write the finally line\n",
        "        elif inside_finally and line.strip() == '':\n",
        "            inside_finally = False  # end of finally block\n",
        "            file.write(line)\n",
        "        elif inside_finally:\n",
        "            if \"cv2.destroyAllWindows()\" in line:\n",
        "                # Replace cv2.destroyAllWindows() with pass, ensuring proper indentation\n",
        "                file.write('         pass\\n')  # Ensuring 'pass' is indented correctly\n",
        "            else:\n",
        "                # Indent other lines inside finally block\n",
        "                file.write('    ' + line)\n",
        "        else:\n",
        "            file.write(line)\n"
      ],
      "metadata": {
        "id": "zl-L-NDwglcp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-display the script to ensure the change is applied\n",
        "!cat -n test_LPRNet.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHcKDIEjgn2N",
        "outputId": "5f2044f8-ca78-4c66-9b07-a888b9149c06"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     1\t# -*- coding: utf-8 -*-\n",
            "     2\t# /usr/bin/env/python3\n",
            "     3\t\n",
            "     4\t'''\n",
            "     5\ttest pretrained model.\n",
            "     6\tAuthor: aiboy.wei@outlook.com .\n",
            "     7\t'''\n",
            "     8\t\n",
            "     9\tfrom data.load_data import CHARS, CHARS_DICT, LPRDataLoader\n",
            "    10\tfrom PIL import Image, ImageDraw, ImageFont\n",
            "    11\tfrom model.LPRNet import build_lprnet\n",
            "    12\t# import torch.backends.cudnn as cudnn\n",
            "    13\tfrom torch.autograd import Variable\n",
            "    14\timport torch.nn.functional as F\n",
            "    15\tfrom torch.utils.data import *\n",
            "    16\tfrom torch import optim\n",
            "    17\timport torch.nn as nn\n",
            "    18\timport numpy as np\n",
            "    19\timport argparse\n",
            "    20\timport torch\n",
            "    21\timport time\n",
            "    22\timport cv2\n",
            "    23\timport os\n",
            "    24\t\n",
            "    25\tdef get_parser():\n",
            "    26\t    parser = argparse.ArgumentParser(description='parameters to train net')\n",
            "    27\t    parser.add_argument('--img_size', default=[94, 24], help='the image size')\n",
            "    28\t    parser.add_argument('--test_img_dirs', default=\"./data/test\", help='the test images path')\n",
            "    29\t    parser.add_argument('--dropout_rate', default=0, help='dropout rate.')\n",
            "    30\t    parser.add_argument('--lpr_max_len', default=8, help='license plate number max length.')\n",
            "    31\t    parser.add_argument('--test_batch_size', default=100, help='testing batch size.')\n",
            "    32\t    parser.add_argument('--phase_train', default=False, type=bool, help='train or test phase flag.')\n",
            "    33\t    parser.add_argument('--num_workers', default=8, type=int, help='Number of workers used in dataloading')\n",
            "    34\t    parser.add_argument('--cuda', default=True, type=bool, help='Use cuda to train model')\n",
            "    35\t    parser.add_argument('--show', default=False, type=bool, help='show test image and its predict result or not.')\n",
            "    36\t    parser.add_argument('--pretrained_model', default='./weights/Final_LPRNet_model.pth', help='pretrained base model')\n",
            "    37\t\n",
            "    38\t    args = parser.parse_args()\n",
            "    39\t\n",
            "    40\t    return args\n",
            "    41\t\n",
            "    42\tdef collate_fn(batch):\n",
            "    43\t    imgs = []\n",
            "    44\t    labels = []\n",
            "    45\t    lengths = []\n",
            "    46\t    for _, sample in enumerate(batch):\n",
            "    47\t        img, label, length = sample\n",
            "    48\t        imgs.append(torch.from_numpy(img))\n",
            "    49\t        labels.extend(label)\n",
            "    50\t        lengths.append(length)\n",
            "    51\t    labels = np.asarray(labels).flatten().astype(np.float32)\n",
            "    52\t\n",
            "    53\t    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)\n",
            "    54\t\n",
            "    55\tdef test():\n",
            "    56\t    args = get_parser()\n",
            "    57\t\n",
            "    58\t    lprnet = build_lprnet(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
            "    59\t    device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
            "    60\t    lprnet.to(device)\n",
            "    61\t    print(\"Successful to build network!\")\n",
            "    62\t\n",
            "    63\t    # load pretrained model\n",
            "    64\t    if args.pretrained_model:\n",
            "    65\t        lprnet.load_state_dict(torch.load(args.pretrained_model))\n",
            "    66\t        print(\"load pretrained model successful!\")\n",
            "    67\t    else:\n",
            "    68\t        print(\"[Error] Can't found pretrained mode, please check!\")\n",
            "    69\t        return False\n",
            "    70\t\n",
            "    71\t    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
            "    72\t    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
            "    73\t    try:\n",
            "    74\t        Greedy_Decode_Eval(lprnet, test_dataset, args)\n",
            "    75\t    finally:\n",
            "    76\t         pass\n",
            "    77\t\n",
            "    78\tdef Greedy_Decode_Eval(Net, datasets, args):\n",
            "    79\t    # TestNet = Net.eval()\n",
            "    80\t    epoch_size = len(datasets) // args.test_batch_size\n",
            "    81\t    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
            "    82\t\n",
            "    83\t    Tp = 0\n",
            "    84\t    Tn_1 = 0\n",
            "    85\t    Tn_2 = 0\n",
            "    86\t    t1 = time.time()\n",
            "    87\t    for i in range(epoch_size):\n",
            "    88\t        # load train data\n",
            "    89\t        images, labels, lengths = next(batch_iterator)\n",
            "    90\t        start = 0\n",
            "    91\t        targets = []\n",
            "    92\t        for length in lengths:\n",
            "    93\t            label = labels[start:start+length]\n",
            "    94\t            targets.append(label)\n",
            "    95\t            start += length\n",
            "    96\t        targets = np.array([el.numpy() for el in targets])\n",
            "    97\t        imgs = images.numpy().copy()\n",
            "    98\t\n",
            "    99\t        if args.cuda:\n",
            "   100\t            images = Variable(images.cuda())\n",
            "   101\t        else:\n",
            "   102\t            images = Variable(images)\n",
            "   103\t\n",
            "   104\t        # forward\n",
            "   105\t        prebs = Net(images)\n",
            "   106\t        # greedy decode\n",
            "   107\t        prebs = prebs.cpu().detach().numpy()\n",
            "   108\t        preb_labels = list()\n",
            "   109\t        for i in range(prebs.shape[0]):\n",
            "   110\t            preb = prebs[i, :, :]\n",
            "   111\t            preb_label = list()\n",
            "   112\t            for j in range(preb.shape[1]):\n",
            "   113\t                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
            "   114\t            no_repeat_blank_label = list()\n",
            "   115\t            pre_c = preb_label[0]\n",
            "   116\t            if pre_c != len(CHARS) - 1:\n",
            "   117\t                no_repeat_blank_label.append(pre_c)\n",
            "   118\t            for c in preb_label: # dropout repeate label and blank label\n",
            "   119\t                if (pre_c == c) or (c == len(CHARS) - 1):\n",
            "   120\t                    if c == len(CHARS) - 1:\n",
            "   121\t                        pre_c = c\n",
            "   122\t                    continue\n",
            "   123\t                no_repeat_blank_label.append(c)\n",
            "   124\t                pre_c = c\n",
            "   125\t            preb_labels.append(no_repeat_blank_label)\n",
            "   126\t        for i, label in enumerate(preb_labels):\n",
            "   127\t            # show image and its predict label\n",
            "   128\t            if args.show:\n",
            "   129\t                show(imgs[i], label, targets[i])\n",
            "   130\t            if len(label) != len(targets[i]):\n",
            "   131\t                Tn_1 += 1\n",
            "   132\t                continue\n",
            "   133\t            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
            "   134\t                Tp += 1\n",
            "   135\t            else:\n",
            "   136\t                Tn_2 += 1\n",
            "   137\t    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
            "   138\t    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp+Tn_1+Tn_2)))\n",
            "   139\t    t2 = time.time()\n",
            "   140\t    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))\n",
            "   141\t\n",
            "   142\tdef show(img, label, target):\n",
            "   143\t    img = np.transpose(img, (1, 2, 0))\n",
            "   144\t    img *= 128.\n",
            "   145\t    img += 127.5\n",
            "   146\t    img = img.astype(np.uint8)\n",
            "   147\t\n",
            "   148\t    lb = \"\"\n",
            "   149\t    for i in label:\n",
            "   150\t        lb += CHARS[i]\n",
            "   151\t    tg = \"\"\n",
            "   152\t    for j in target.tolist():\n",
            "   153\t        tg += CHARS[int(j)]\n",
            "   154\t\n",
            "   155\t    flag = \"F\"\n",
            "   156\t    if lb == tg:\n",
            "   157\t        flag = \"T\"\n",
            "   158\t    # img = cv2.putText(img, lb, (0,16), cv2.FONT_HERSHEY_COMPLEX_SMALL, 0.6, (0, 0, 255), 1)\n",
            "   159\t    img = cv2ImgAddText(img, lb, (0, 0))\n",
            "   160\t    cv2.imshow(\"test\", img)\n",
            "   161\t    print(\"target: \", tg, \" ### {} ### \".format(flag), \"predict: \", lb)\n",
            "   162\t    cv2.waitKey()\n",
            "   163\t    cv2.destroyAllWindows()\n",
            "   164\t\n",
            "   165\tdef cv2ImgAddText(img, text, pos, textColor=(255, 0, 0), textSize=12):\n",
            "   166\t    if (isinstance(img, np.ndarray)):  # detect opencv format or not\n",
            "   167\t        img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
            "   168\t    draw = ImageDraw.Draw(img)\n",
            "   169\t    fontText = ImageFont.truetype(\"data/NotoSansCJK-Regular.ttc\", textSize, encoding=\"utf-8\")\n",
            "   170\t    draw.text(pos, text, textColor, font=fontText)\n",
            "   171\t\n",
            "   172\t    return cv2.cvtColor(np.asarray(img), cv2.COLOR_RGB2BGR)\n",
            "   173\t\n",
            "   174\t\n",
            "   175\tif __name__ == \"__main__\":\n",
            "   176\t    test()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_LPRNet.py --test_img_dirs ./data/test --pretrained_model ./weights/Final_LPRNet_model.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vbNdH1Jgr2m",
        "outputId": "65a0501d-61b0-467e-9ee8-b72c70d4e86e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n",
            "/content/LPRNet_Pytorch/test_LPRNet.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model))\n",
            "load pretrained model successful!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.901 [901:57:42:1000]\n",
            "[Info] Test Speed: 0.0008513927459716797s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model.LPRNet import build_lprnet\n",
        "from data.load_data import CHARS, CHARS_DICT, LPRDataLoader\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Load the LPRNet model\n",
        "lprnet = build_lprnet(lpr_max_len=8, phase=False, class_num=len(CHARS), dropout_rate=0.5)\n",
        "lprnet.to(device)\n",
        "weights_path='/content/LPRNet_Pytorch/weights/Final_LPRNet_model.pth'\n",
        "# Load pre-trained weights\n",
        "lprnet.load_state_dict(torch.load(weights_path,weights_only=True))\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdnij68pkfEd",
        "outputId": "a577ac65-0a79-4a69-9963-43ba716eae10"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. PRUNING"
      ],
      "metadata": {
        "id": "XnU8UCGhgdIR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils.prune as prune\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from imutils import paths\n",
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "import time\n",
        "\n",
        "\n",
        "# Pruning Function\n",
        "def apply_pruning(model, pruning_percentage=0.3, device='cuda'):\n",
        "    model.to(device)\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "            print(f\"Pruning layer: {name}\")\n",
        "            prune.l1_unstructured(module, name=\"weight\", amount=pruning_percentage)\n",
        "            prune.remove(module, 'weight')  # Make pruning permanent\n",
        "    print(\"Pruning complete!\")\n",
        "    return model\n",
        "\n",
        "# Character Set for License Plates\n",
        "CHARS = ['京', '沪', '津', '渝', '冀', '晋', '蒙', '辽', '吉', '黑',\n",
        "         '苏', '浙', '皖', '闽', '赣', '鲁', '豫', '鄂', '湘', '粤',\n",
        "         '桂', '琼', '川', '贵', '云', '藏', '陕', '甘', '青', '宁',\n",
        "         '新', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "         'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K',\n",
        "         'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V',\n",
        "         'W', 'X', 'Y', 'Z', 'I', 'O', '-']\n",
        "CHARS_DICT = {char: i for i, char in enumerate(CHARS)}\n",
        "\n",
        "# Dataset Class\n",
        "class LPRDataLoader(Dataset):\n",
        "    def __init__(self, img_dir, imgSize, lpr_max_len, PreprocFun=None):\n",
        "        self.img_dir = img_dir\n",
        "        self.img_paths = []\n",
        "        for dir_path in img_dir:\n",
        "            self.img_paths += [el for el in paths.list_images(dir_path)]\n",
        "        random.shuffle(self.img_paths)\n",
        "        self.img_size = imgSize\n",
        "        self.lpr_max_len = lpr_max_len\n",
        "        self.PreprocFun = PreprocFun if PreprocFun else self.transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.img_paths[index]\n",
        "        image = cv2.imread(filename)\n",
        "        image = cv2.resize(image, self.img_size)\n",
        "        image = self.PreprocFun(image)\n",
        "\n",
        "        basename = os.path.basename(filename)\n",
        "        imgname = basename.split(\"-\")[0].split(\"_\")[0]\n",
        "\n",
        "        # Filter out invalid characters\n",
        "        imgname = ''.join([c for c in imgname if c in CHARS_DICT])\n",
        "\n",
        "        # Convert valid characters to labels\n",
        "        label = [CHARS_DICT[c] for c in imgname]\n",
        "\n",
        "        return torch.tensor(image, dtype=torch.float32), torch.tensor(label, dtype=torch.long), len(label)\n",
        "\n",
        "    def transform(self, img):\n",
        "        img = img.astype('float32')\n",
        "        img -= 127.5\n",
        "        img *= 0.0078125\n",
        "        img = np.transpose(img, (2, 0, 1))  # Convert to (C, H, W)\n",
        "        return img\n",
        "\n",
        "# # Evaluation Function\n",
        "# def greedy_decode_evaluate(model, test_loader, device='cuda'):\n",
        "#     model.eval()\n",
        "#     correct, total = 0, 0\n",
        "#     with torch.no_grad():\n",
        "#         for data in test_loader:\n",
        "#             inputs, labels, lengths = data\n",
        "#             inputs = inputs.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             # Perform model inference\n",
        "#             outputs = model(inputs).cpu().numpy()\n",
        "\n",
        "#             for i, preb in enumerate(outputs):\n",
        "#                 preb_label = [np.argmax(preb[:, j]) for j in range(preb.shape[1])]\n",
        "\n",
        "#                 # Greedy decoding: Remove repeated and blank tokens\n",
        "#                 no_repeat_blank_label = []\n",
        "#                 prev_c = preb_label[0]\n",
        "#                 if prev_c != len(CHARS) - 1:\n",
        "#                     no_repeat_blank_label.append(prev_c)\n",
        "#                 for c in preb_label:\n",
        "#                     if c != prev_c and c != len(CHARS) - 1:\n",
        "#                         no_repeat_blank_label.append(c)\n",
        "#                     prev_c = c\n",
        "\n",
        "#                 # Compare decoded prediction with ground truth\n",
        "#                 gt_label = labels[i][:lengths[i]].tolist()\n",
        "#                 if no_repeat_blank_label == gt_label:\n",
        "#                     correct += 1\n",
        "#                 total += 1\n",
        "\n",
        "#     # Calculate accuracy\n",
        "#     accuracy = 100.0 * correct / total if total > 0 else 0.0\n",
        "#     return accuracy\n",
        "\n",
        "\n",
        "# Fix for shape mismatch while loading pre-trained weights\n",
        "def load_pretrained_weights(model, pretrained_model_path):\n",
        "    checkpoint = torch.load(pretrained_model_path,weights_only=True)\n",
        "    model_state_dict = model.state_dict()\n",
        "\n",
        "    # List of layers to update based on matching shapes\n",
        "    for key in checkpoint.keys():\n",
        "        if key in model_state_dict and checkpoint[key].shape == model_state_dict[key].shape:\n",
        "            model_state_dict[key] = checkpoint[key]\n",
        "        else:\n",
        "            print(f\"Skipping layer {key} due to shape mismatch\")\n",
        "\n",
        "    model.load_state_dict(model_state_dict)\n",
        "    print(\"Pre-trained weights loaded successfully (with shape mismatch handling).\")\n",
        "\n",
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "\n",
        "# Function to Load Test Data\n",
        "def get_test_loader(test_folder, batch_size=32):\n",
        "    test_dataset = LPRDataLoader(test_folder.split(','),imgSize=(94, 24),lpr_max_len='8')\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    return test_loader\n"
      ],
      "metadata": {
        "id": "pf_lGZ2PgfLC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Testing Code\n",
        "if __name__ == '__main__':\n",
        "    test_folder = \"/content/LPRNet_Pytorch/data/test\"  # Path to your test dataset\n",
        "    pretrained_model_path = weights_path # Path to pretrained weights\n",
        "\n",
        "    # Initialize your LPRNet model\n",
        "    lprnet = build_lprnet(class_num=68, dropout_rate=0.5)  # Replace with LPRNet initialization\n",
        "\n",
        "    # Load Pre-trained Weights\n",
        "    load_pretrained_weights(lprnet, pretrained_model_path)\n",
        "\n",
        "    # Get Test DataLoader\n",
        "    test_loader = get_test_loader(test_folder, batch_size=100)\n",
        "\n",
        "    # Prune the Model\n",
        "    unstructured_pruning = apply_pruning(lprnet, pruning_percentage=0.3, device='cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AIeLJgnSeUvg",
        "outputId": "23978d31-bd7c-4a15-997d-16a7561533cd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained weights loaded successfully (with shape mismatch handling).\n",
            "Pruning layer: backbone.0\n",
            "Pruning layer: backbone.4.block.0\n",
            "Pruning layer: backbone.4.block.2\n",
            "Pruning layer: backbone.4.block.4\n",
            "Pruning layer: backbone.4.block.6\n",
            "Pruning layer: backbone.8.block.0\n",
            "Pruning layer: backbone.8.block.2\n",
            "Pruning layer: backbone.8.block.4\n",
            "Pruning layer: backbone.8.block.6\n",
            "Pruning layer: backbone.11.block.0\n",
            "Pruning layer: backbone.11.block.2\n",
            "Pruning layer: backbone.11.block.4\n",
            "Pruning layer: backbone.11.block.6\n",
            "Pruning layer: backbone.16\n",
            "Pruning layer: backbone.20\n",
            "Pruning layer: container.0\n",
            "Pruning complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(unstructured_pruning.state_dict(), './weights/pruned_model_weights_trial2.pth')\n"
      ],
      "metadata": {
        "id": "-d3SJZcFnN8e"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Speed After Pruning"
      ],
      "metadata": {
        "id": "P9oRBMxRgeKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python test_LPRNet.py --pretrained_model ./weights/pruned_model_weights_trial2.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MQkslTznSo8",
        "outputId": "17d0e0ab-674e-45e3-8dae-3dc8811abb97"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n",
            "/content/LPRNet_Pytorch/test_LPRNet.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model))\n",
            "load pretrained model successful!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.885 [885:71:44:1000]\n",
            "[Info] Test Speed: 0.0008252706527709961s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Size After Pruning"
      ],
      "metadata": {
        "id": "Oinpg0r0kqv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from model.LPRNet import build_lprnet  # Ensure this matches your file path\n",
        "\n",
        "\n",
        "\n",
        "# Now you can export the wrapped model to ONNX\n",
        "dummy_input = torch.randn(1, 3, 24, 94,device='cuda')  # Dummy input for ONNX export\n",
        "unstructured_pruning.to(dummy_input.device)\n",
        "pruned_onnx_model_path = './weights/pruned_lprnet_model.onnx'\n",
        "torch.onnx.export(unstructured_pruning, dummy_input, pruned_onnx_model_path, weight_type=QuantType.QInt8)\n",
        "print(f'Model exported to: {pruned_onnx_model_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-doSvMokpEl",
        "outputId": "0d0784e8-dca8-443c-d4c5-11004dd19ced"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to: ./weights/pruned_lprnet_model.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the file size of the ONNX model\n",
        "unstructuredpruning_onnx_size = os.path.getsize(pruned_onnx_model_path) / (1024 * 1024)  # Size in MB\n",
        "print(f\"Pruned ONNX model size: {unstructuredpruning_onnx_size:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToeWK2IQkurp",
        "outputId": "3783043e-82db-4295-bc9a-39a73c029ea8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned ONNX model size: 1.71 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization"
      ],
      "metadata": {
        "id": "M_F_xK4BkxpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.quantization import quantize_dynamic\n",
        "\n",
        "def dynamic_quantize(lprnet):\n",
        "  quantized_lprnet = quantize_dynamic(lprnet, {nn.Conv2d, nn.Linear}, dtype=torch.qint8)\n",
        "  torch.save(quantized_lprnet.state_dict(), \"quantized_lprnet.pth\")\n",
        "  return quantized_lprnet"
      ],
      "metadata": {
        "id": "0TSl4ZjJkz3C"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Testing Code\n",
        "if __name__ == '__main__':\n",
        "    test_folder = \"/content/LPRNet_Pytorch/data/test\"  # Path to your test dataset\n",
        "    pretrained_model_path = weights_path # Path to pretrained weights\n",
        "\n",
        "    # Initialize your LPRNet model\n",
        "    lprnet = build_lprnet(class_num=68, dropout_rate=0.5)  # Replace with LPRNet initialization\n",
        "\n",
        "    # Load Pre-trained Weights\n",
        "    load_pretrained_weights(lprnet, pretrained_model_path)\n",
        "\n",
        "    # Get Test DataLoader\n",
        "    test_loader = get_test_loader(test_folder, batch_size=100)\n",
        "    # Prune the Model\n",
        "    dynamic_quantized = dynamic_quantize(lprnet)\n",
        "    dynamic_quantized.to('cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9BoqRCKk2G-",
        "outputId": "907488cb-68bc-4f21-a0bb-286ac3e42c41"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained weights loaded successfully (with shape mismatch handling).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fuse_lprnet_corrected(model):\n",
        "    \"\"\"\n",
        "    Optimize the LPRNet model by fusing only valid layer combinations.\n",
        "\n",
        "    Args:\n",
        "        model (torch.nn.Module): The LPRNet model to optimize.\n",
        "\n",
        "    Returns:\n",
        "        torch.nn.Module: The optimized and fused LPRNet model.\n",
        "    \"\"\"\n",
        "\n",
        "    def fuse_sequential_layers(sequential_block, block_name):\n",
        "        \"\"\"\n",
        "        Fuse eligible layer combinations in a sequential block.\n",
        "\n",
        "        Args:\n",
        "            sequential_block (nn.Sequential): A sequential block of layers.\n",
        "            block_name (str): The name of the block for debugging/logging.\n",
        "        \"\"\"\n",
        "        valid_fusions = []\n",
        "        for name, layer in sequential_block.named_children():\n",
        "          if isinstance(layer, nn.Conv2d) or (\n",
        "             valid_fusions and isinstance(layer, (nn.BatchNorm2d, nn.ReLU))\n",
        "           ):\n",
        "            valid_fusions.append(name)\n",
        "          else:\n",
        "            if len(valid_fusions) >= 2:\n",
        "              torch.quantization.fuse_modules(sequential_block, valid_fusions, inplace=True)\n",
        "              print(f\"Fused layers in block {block_name}: {valid_fusions}\")\n",
        "            valid_fusions = []  # Reset\n",
        "        if len(valid_fusions) >= 2:\n",
        "          torch.quantization.fuse_modules(sequential_block, valid_fusions, inplace=True)\n",
        "          print(f\"Fused layers in block {block_name}: {valid_fusions}\")\n",
        "\n",
        "    def traverse_and_fuse(module, module_name=\"\"):\n",
        "        \"\"\"\n",
        "        Recursively traverse and apply fusion on modules.\n",
        "\n",
        "        Args:\n",
        "            module (torch.nn.Module): Current module to process.\n",
        "            module_name (str): Parent module name for debugging/logging.\n",
        "        \"\"\"\n",
        "        if isinstance(module, nn.Sequential):\n",
        "            fuse_sequential_layers(module, module_name)\n",
        "        elif hasattr(module, \"children\"):\n",
        "            for name, child in module.named_children():\n",
        "                full_name = f\"{module_name}.{name}\" if module_name else name\n",
        "                traverse_and_fuse(child, full_name)\n",
        "\n",
        "    # Fuse layers in backbone\n",
        "    if hasattr(model, \"backbone\"):\n",
        "        print(\"Fusing backbone layers...\")\n",
        "        traverse_and_fuse(model.backbone, \"backbone\")\n",
        "\n",
        "    # Fuse layers in container (if present)\n",
        "    if hasattr(model, \"container\"):\n",
        "        print(\"Fusing container layers...\")\n",
        "        fuse_sequential_layers(model.container, \"container\")\n",
        "\n",
        "    print(\"Fusion process completed.\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "DmcDpdQDk5jN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.ao.quantization import prepare, convert\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize your LPRNet model\n",
        "    lprnet = build_lprnet(class_num=68, dropout_rate=0.5)  # Replace with LPRNet initialization\n",
        "\n",
        "    # Load Pre-trained Weights\n",
        "    load_pretrained_weights(lprnet, pretrained_model_path)\n",
        "\n",
        "    # Load the data\n",
        "    test_loader = get_test_loader(test_folder, batch_size=100)\n",
        "    lprnet.to('cuda')\n",
        "    fused_lprnet = fuse_lprnet_corrected(lprnet)\n",
        "    fused_lprnet.eval()\n",
        "    fused_lprnet.qconfig = torch.quantization.get_default_qconfig('qnnpack')  # or 'qnnpack' for mobile\n",
        "    torch.quantization.prepare(fused_lprnet, inplace=True)\n",
        "    torch.quantization.convert(fused_lprnet, inplace=False)\n",
        "\n",
        "    torch.save(fused_lprnet.state_dict(), \"fused_lprnet_quantized_lprnet.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VovH6rhk7Zw",
        "outputId": "8aae406f-2aad-4616-98ee-887605a2110d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pre-trained weights loaded successfully (with shape mismatch handling).\n",
            "Fusing backbone layers...\n",
            "Fused layers in block backbone: ['0', '1', '2']\n",
            "Fused layers in block backbone: ['16', '17', '18']\n",
            "Fused layers in block backbone: ['20', '21', '22']\n",
            "Fusing container layers...\n",
            "Fusion process completed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/ao/quantization/observer.py:1315: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from model.LPRNet import build_lprnet  # Ensure this matches your file path\n",
        "\n",
        "\n",
        "\n",
        "# Now you can export the wrapped model to ONNX\n",
        "dummy_input = torch.randn(1, 3, 24, 94,device='cuda')  # Dummy input for ONNX export\n",
        "fused_lprnet.to(dummy_input.device)\n",
        "quantized_onnx_model_path = './weights/fused_lprnet_quantized_lprnet.pth'\n",
        "torch.onnx.export(fused_lprnet, dummy_input, quantized_onnx_model_path, weight_type=QuantType.QInt8)\n",
        "print(f'Model exported to: {quantized_onnx_model_path}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dq4s58yqfU7o",
        "outputId": "57d4b7c5-b4e9-419a-9d26-b8f62b7b32ec"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported to: ./weights/fused_lprnet_quantized_lprnet.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the file size of the ONNX model\n",
        "quantized_onnx_size = os.path.getsize(quantized_onnx_model_path) / (1024 * 1024)  # Size in MB\n",
        "print(f\"ONNX model size: {quantized_onnx_size:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZU-HXWnfjvy",
        "outputId": "5c2fd327-5e01-475d-caa5-8731f698adca"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX model size: 1.71 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speed after Quantization"
      ],
      "metadata": {
        "id": "qb6PCjMMrI5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python test_LPRNet.py --pretrained_model ./quantized_lprnet.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlHlKriVnqKw",
        "outputId": "b2fe83ca-0a76-44a9-efb2-25a5ba4942a1"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n",
            "/content/LPRNet_Pytorch/test_LPRNet.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model))\n",
            "load pretrained model successful!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.899 [899:60:41:1000]\n",
            "[Info] Test Speed: 0.0008352820873260498s 1/1000]\n"
          ]
        }
      ]
    }
  ]
}